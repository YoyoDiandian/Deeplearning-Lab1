{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e4428e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert-base-uncased and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading data...\n",
      "文件 training.csv 中的标签分布:\n",
      "唯一标签值: [0 1 2 3 4 5]\n",
      "标签计数: {np.int64(0): 4665, np.int64(1): 5362, np.int64(2): 1304, np.int64(3): 2159, np.int64(4): 1937, np.int64(5): 572}\n",
      "文件 validation.csv 中的标签分布:\n",
      "唯一标签值: [0 1 2 3 4 5]\n",
      "标签计数: {np.int64(0): 549, np.int64(1): 704, np.int64(2): 178, np.int64(3): 275, np.int64(4): 212, np.int64(5): 81}\n",
      "检测到的最大标签值: 5，设置num_labels=6\n",
      "Starting training...\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch 40 of 1000. Elapsed: 0:00:53\n",
      "  Batch 80 of 1000. Elapsed: 0:01:42\n",
      "  Batch 120 of 1000. Elapsed: 0:02:30\n",
      "  Batch 160 of 1000. Elapsed: 0:03:19\n",
      "  Batch 200 of 1000. Elapsed: 0:04:08\n",
      "  Batch 240 of 1000. Elapsed: 0:04:58\n",
      "  Batch 280 of 1000. Elapsed: 0:05:47\n",
      "  Batch 320 of 1000. Elapsed: 0:06:36\n",
      "  Batch 360 of 1000. Elapsed: 0:07:25\n",
      "  Batch 400 of 1000. Elapsed: 0:08:14\n",
      "  Batch 440 of 1000. Elapsed: 0:09:06\n",
      "  Batch 480 of 1000. Elapsed: 0:09:54\n",
      "  Batch 520 of 1000. Elapsed: 0:10:44\n",
      "  Batch 560 of 1000. Elapsed: 0:11:32\n",
      "  Batch 600 of 1000. Elapsed: 0:12:20\n",
      "  Batch 640 of 1000. Elapsed: 0:13:09\n",
      "  Batch 680 of 1000. Elapsed: 0:13:58\n",
      "  Batch 720 of 1000. Elapsed: 0:14:47\n",
      "  Batch 760 of 1000. Elapsed: 0:15:35\n",
      "  Batch 800 of 1000. Elapsed: 0:16:23\n",
      "  Batch 840 of 1000. Elapsed: 0:17:10\n",
      "  Batch 880 of 1000. Elapsed: 0:17:59\n",
      "  Batch 920 of 1000. Elapsed: 0:18:49\n",
      "  Batch 960 of 1000. Elapsed: 0:19:39\n",
      "  Average training loss: 0.4824\n",
      "  Training epoch took: 0:20:26\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.6083\n",
      "  Validation Loss: 0.1831\n",
      "  Precision: 0.9913\n",
      "  Recall: 0.9927\n",
      "  F1-Score: 0.9920\n",
      "  Validation took: 0:00:36\n",
      "  Best model saved at: model_output/best_model2.pt\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch 40 of 1000. Elapsed: 0:00:49\n",
      "  Batch 80 of 1000. Elapsed: 0:01:37\n",
      "  Batch 120 of 1000. Elapsed: 0:02:26\n",
      "  Batch 160 of 1000. Elapsed: 0:03:15\n",
      "  Batch 200 of 1000. Elapsed: 0:04:03\n",
      "  Batch 240 of 1000. Elapsed: 0:04:52\n",
      "  Batch 280 of 1000. Elapsed: 0:05:42\n",
      "  Batch 320 of 1000. Elapsed: 0:06:30\n",
      "  Batch 360 of 1000. Elapsed: 0:07:18\n",
      "  Batch 400 of 1000. Elapsed: 0:08:05\n",
      "  Batch 440 of 1000. Elapsed: 0:08:53\n",
      "  Batch 480 of 1000. Elapsed: 0:09:42\n",
      "  Batch 520 of 1000. Elapsed: 0:10:30\n",
      "  Batch 560 of 1000. Elapsed: 0:11:17\n",
      "  Batch 600 of 1000. Elapsed: 0:12:03\n",
      "  Batch 640 of 1000. Elapsed: 0:12:51\n",
      "  Batch 680 of 1000. Elapsed: 0:13:39\n",
      "  Batch 720 of 1000. Elapsed: 0:14:28\n",
      "  Batch 760 of 1000. Elapsed: 0:15:18\n",
      "  Batch 800 of 1000. Elapsed: 0:16:05\n",
      "  Batch 840 of 1000. Elapsed: 0:16:52\n",
      "  Batch 880 of 1000. Elapsed: 0:17:44\n",
      "  Batch 920 of 1000. Elapsed: 0:18:36\n",
      "  Batch 960 of 1000. Elapsed: 0:19:24\n",
      "  Average training loss: 0.1530\n",
      "  Training epoch took: 0:20:12\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.5938\n",
      "  Validation Loss: 0.1469\n",
      "  Precision: 0.9954\n",
      "  Recall: 0.9939\n",
      "  F1-Score: 0.9947\n",
      "  Validation took: 0:00:36\n",
      "  Best model saved at: model_output/best_model2.pt\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch 40 of 1000. Elapsed: 0:00:49\n",
      "  Batch 80 of 1000. Elapsed: 0:01:38\n",
      "  Batch 120 of 1000. Elapsed: 0:02:26\n",
      "  Batch 160 of 1000. Elapsed: 0:03:14\n",
      "  Batch 200 of 1000. Elapsed: 0:04:02\n",
      "  Batch 240 of 1000. Elapsed: 0:04:50\n",
      "  Batch 280 of 1000. Elapsed: 0:05:38\n",
      "  Batch 320 of 1000. Elapsed: 0:06:28\n",
      "  Batch 360 of 1000. Elapsed: 0:07:17\n",
      "  Batch 400 of 1000. Elapsed: 0:08:05\n",
      "  Batch 440 of 1000. Elapsed: 0:08:53\n",
      "  Batch 480 of 1000. Elapsed: 0:09:41\n",
      "  Batch 520 of 1000. Elapsed: 0:10:29\n",
      "  Batch 560 of 1000. Elapsed: 0:11:17\n",
      "  Batch 600 of 1000. Elapsed: 0:12:06\n",
      "  Batch 640 of 1000. Elapsed: 0:12:55\n",
      "  Batch 680 of 1000. Elapsed: 0:13:45\n",
      "  Batch 720 of 1000. Elapsed: 0:14:34\n",
      "  Batch 760 of 1000. Elapsed: 0:15:23\n",
      "  Batch 800 of 1000. Elapsed: 0:16:12\n",
      "  Batch 840 of 1000. Elapsed: 0:17:00\n",
      "  Batch 880 of 1000. Elapsed: 0:17:48\n",
      "  Batch 920 of 1000. Elapsed: 0:18:37\n",
      "  Batch 960 of 1000. Elapsed: 0:19:26\n",
      "  Average training loss: 0.1019\n",
      "  Training epoch took: 0:20:15\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.6083\n",
      "  Validation Loss: 0.1598\n",
      "  Precision: 0.9956\n",
      "  Recall: 0.9942\n",
      "  F1-Score: 0.9949\n",
      "  Validation took: 0:00:36\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch 40 of 1000. Elapsed: 0:00:49\n",
      "  Batch 80 of 1000. Elapsed: 0:01:37\n",
      "  Batch 120 of 1000. Elapsed: 0:02:26\n",
      "  Batch 160 of 1000. Elapsed: 0:03:15\n",
      "  Batch 200 of 1000. Elapsed: 0:04:04\n",
      "  Batch 240 of 1000. Elapsed: 0:04:52\n",
      "  Batch 280 of 1000. Elapsed: 0:05:42\n",
      "  Batch 320 of 1000. Elapsed: 0:06:32\n",
      "  Batch 360 of 1000. Elapsed: 0:07:20\n",
      "  Batch 400 of 1000. Elapsed: 0:08:07\n",
      "  Batch 440 of 1000. Elapsed: 0:08:56\n",
      "  Batch 480 of 1000. Elapsed: 0:09:44\n",
      "  Batch 520 of 1000. Elapsed: 0:10:32\n",
      "  Batch 560 of 1000. Elapsed: 0:11:20\n",
      "  Batch 600 of 1000. Elapsed: 0:12:07\n",
      "  Batch 640 of 1000. Elapsed: 0:12:55\n",
      "  Batch 680 of 1000. Elapsed: 0:13:42\n",
      "  Batch 720 of 1000. Elapsed: 0:14:29\n",
      "  Batch 760 of 1000. Elapsed: 0:15:18\n",
      "  Batch 800 of 1000. Elapsed: 0:16:07\n",
      "  Batch 840 of 1000. Elapsed: 0:16:55\n",
      "  Batch 880 of 1000. Elapsed: 0:17:43\n",
      "  Batch 920 of 1000. Elapsed: 0:18:30\n",
      "  Batch 960 of 1000. Elapsed: 0:19:17\n",
      "  Average training loss: 0.0710\n",
      "  Training epoch took: 0:20:06\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.5983\n",
      "  Validation Loss: 0.1808\n",
      "  Precision: 0.9955\n",
      "  Recall: 0.9941\n",
      "  F1-Score: 0.9948\n",
      "  Validation took: 0:00:36\n",
      "\n",
      "Training complete! Total training took 1:23:26\n",
      "Training completed with best validation loss: 0.1469\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(sentences, truncation=True, padding='max_length', max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"设置随机种子，确保结果可复现\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def load_data(file_path, tokenizer, max_length=128):\n",
    "    \"\"\"加载并处理数据，不使用pandas\"\"\"\n",
    "    try:\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            # 跳过标题行\n",
    "            next(f)\n",
    "            \n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                    \n",
    "                # 假设CSV格式为: sentence,label\n",
    "                parts = line.split(',')\n",
    "                if len(parts) >= 2:\n",
    "                    sentence = ','.join(parts[:-1])  # 处理句子中可能包含的逗号\n",
    "                    label = int(parts[-1])\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "        \n",
    "        dataset = TextDataset(sentences, labels, tokenizer, max_length)\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"数据加载错误: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_data_loader(dataset, batch_size, sampler_type='random'):\n",
    "    \"\"\"创建数据加载器\"\"\"\n",
    "    if sampler_type == 'random':\n",
    "        sampler = torch.utils.data.RandomSampler(dataset)\n",
    "    else:\n",
    "        sampler = torch.utils.data.SequentialSampler(dataset)\n",
    "    \n",
    "    return DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "def calculate_accuracy(preds, labels):\n",
    "    \"\"\"计算准确率，不使用sklearn\"\"\"\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def calculate_metrics(preds, labels):\n",
    "    \"\"\"计算精确率、召回率和F1分数，不使用sklearn\"\"\"\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    tp = np.sum((pred_flat == 1) & (labels_flat == 1))\n",
    "    tn = np.sum((pred_flat == 0) & (labels_flat == 0))\n",
    "    fp = np.sum((pred_flat == 1) & (labels_flat == 0))\n",
    "    fn = np.sum((pred_flat == 0) & (labels_flat == 1))\n",
    "    \n",
    "    # 避免除零错误\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': (tp + tn) / len(labels_flat),\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def calculate_metrics_multiclass(preds, labels):\n",
    "    \"\"\"计算多分类任务的指标\"\"\"\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    # 简单的准确率计算\n",
    "    accuracy = np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "    \n",
    "    # 类别统计\n",
    "    classes = np.unique(np.concatenate([pred_flat, labels_flat]))\n",
    "    print(f\"发现的类别: {classes}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': 0,  # 多分类下需要更复杂的计算\n",
    "        'recall': 0,     # 同上\n",
    "        'f1': 0          # 同上\n",
    "    }\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, optimizer, scheduler, device, epochs, save_path):\n",
    "    \"\"\"训练模型\"\"\"\n",
    "    total_t0 = time.time()\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    for epoch_i in range(0, epochs):\n",
    "        print(f'======== Epoch {epoch_i + 1} / {epochs} ========')\n",
    "        print('Training...')\n",
    "        \n",
    "        t0 = time.time()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print(f'  Batch {step} of {len(train_dataloader)}. Elapsed: {elapsed}')\n",
    "            \n",
    "            b_input_ids = batch['input_ids'].to(device)\n",
    "            b_input_mask = batch['attention_mask'].to(device)\n",
    "            b_labels = batch['labels'].to(device)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            \n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask, \n",
    "                            labels=b_labels)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        training_time = format_time(time.time() - t0)\n",
    "        \n",
    "        print(f\"  Average training loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Training epoch took: {training_time}\")\n",
    "        \n",
    "        print(\"\\nRunning Validation...\")\n",
    "        t0 = time.time()\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        total_eval_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for batch in val_dataloader:\n",
    "            b_input_ids = batch['input_ids'].to(device)\n",
    "            b_input_mask = batch['attention_mask'].to(device)\n",
    "            b_labels = batch['labels'].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids, \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=b_input_mask,\n",
    "                                labels=b_labels)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_eval_loss += loss.item()\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            \n",
    "            all_preds.append(logits)\n",
    "            all_labels.append(label_ids)\n",
    "        \n",
    "        all_preds = np.concatenate(all_preds, axis=0)\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        \n",
    "        avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "        metrics = calculate_metrics(all_preds, all_labels)\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        \n",
    "        print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1-Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"  Validation took: {validation_time}\")\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            model_path = os.path.join(save_path, 'best_model2.pt')\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"  Best model saved at: {model_path}\")\n",
    "    \n",
    "    print(f\"\\nTraining complete! Total training took {format_time(time.time()-total_t0)}\")\n",
    "    return best_val_loss\n",
    "\n",
    "def format_time(elapsed):\n",
    "    \"\"\"将时间格式化为 hh:mm:ss\"\"\"\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def inspect_dataset(file_path):\n",
    "    \"\"\"检查数据集中的标签分布\"\"\"\n",
    "    labels = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        next(f)  # 跳过标题行\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            if len(parts) >= 2:\n",
    "                label = int(parts[-1])\n",
    "                labels.append(label)\n",
    "    \n",
    "    unique_labels = np.unique(labels)\n",
    "    counts = {label: labels.count(label) for label in unique_labels}\n",
    "    \n",
    "    print(f\"文件 {file_path} 中的标签分布:\")\n",
    "    print(f\"唯一标签值: {unique_labels}\")\n",
    "    print(f\"标签计数: {counts}\")\n",
    "    \n",
    "    return unique_labels, counts\n",
    "\n",
    "def main():\n",
    "    # 设置参数\n",
    "    SEED = 42\n",
    "    BATCH_SIZE = 16\n",
    "    LEARNING_RATE = 2e-5\n",
    "    EPSILON = 1e-8\n",
    "    EPOCHS = 4\n",
    "    MAX_LENGTH = 128\n",
    "    SAVE_PATH = 'model_output'\n",
    "    TRAIN_FILE = 'training.csv'\n",
    "    VAL_FILE = 'validation.csv'\n",
    "    \n",
    "    set_seed(SEED)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        num_labels=6,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    train_dataset = load_data(TRAIN_FILE, tokenizer, MAX_LENGTH)\n",
    "    val_dataset = load_data(VAL_FILE, tokenizer, MAX_LENGTH)\n",
    "    \n",
    "    if train_dataset is None or val_dataset is None:\n",
    "        print(\"数据加载失败，程序退出\")\n",
    "        return\n",
    "    \n",
    "    train_labels, train_counts = inspect_dataset(TRAIN_FILE)\n",
    "    val_labels, val_counts = inspect_dataset(VAL_FILE)\n",
    "\n",
    "    # 确保模型配置与数据匹配\n",
    "    num_labels = max(max(train_labels), max(val_labels)) + 1\n",
    "    print(f\"检测到的最大标签值: {num_labels-1}，设置num_labels={num_labels}\")\n",
    "\n",
    "    train_dataloader = create_data_loader(train_dataset, BATCH_SIZE, 'random')\n",
    "    val_dataloader = create_data_loader(val_dataset, BATCH_SIZE, 'sequential')\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=EPSILON)\n",
    "    total_steps = len(train_dataloader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps=0,\n",
    "                                                num_training_steps=total_steps)\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    best_val_loss = train_model(model, train_dataloader, val_dataloader, optimizer, scheduler, \n",
    "                                device, EPOCHS, SAVE_PATH)\n",
    "    \n",
    "    print(f\"Training completed with best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53d5a983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yoyowang/Documents/Study/Junior_B/深度学习/Deeplearning-Lab1/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "模型权重已加载: model_output/best_model2.pt\n",
      "Evaluating model...\n",
      "\n",
      "Evaluation Results:\n",
      "Accuracy: 0.6033\n",
      "Precision: 0.9969\n",
      "Recall: 1.0000\n",
      "F1-Score: 0.9984\n",
      "\n",
      "Example Predictions:\n",
      "\n",
      "Text: This is a great product!\n",
      "Predicted Class: 1\n",
      "Confidence: 0.9974\n",
      "Probabilities: [1.6023201e-04 9.9740666e-01 1.8177849e-03 1.9936159e-04 1.0628299e-04\n",
      " 3.0962826e-04]\n",
      "\n",
      "Text: I did not like the service at all.\n",
      "Predicted Class: 3\n",
      "Confidence: 0.5788\n",
      "Probabilities: [0.08676189 0.2030707  0.10872705 0.5788078  0.01574844 0.00688421]\n",
      "\n",
      "Text: The food was delicious and the staff were friendly.\n",
      "Predicted Class: 1\n",
      "Confidence: 0.9942\n",
      "Probabilities: [1.3832860e-04 9.9415404e-01 5.0942395e-03 2.8245529e-04 9.3238341e-05\n",
      " 2.3778724e-04]\n",
      "\n",
      "Text: I will never come back to this place again.\n",
      "Predicted Class: 0\n",
      "Confidence: 0.8965\n",
      "Probabilities: [0.8965042  0.01780757 0.00754229 0.06316152 0.0139906  0.00099382]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import DataLoader  # 添加这一行\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(sentences, truncation=True, padding='max_length', max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def load_data(file_path, tokenizer, max_length=128):\n",
    "    \"\"\"加载并处理数据，不使用pandas\"\"\"\n",
    "    try:\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            # 跳过标题行\n",
    "            next(f)\n",
    "            \n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                    \n",
    "                # 假设CSV格式为: sentence,label\n",
    "                parts = line.split(',')\n",
    "                if len(parts) >= 2:\n",
    "                    sentence = ','.join(parts[:-1])  # 处理句子中可能包含的逗号\n",
    "                    label = int(parts[-1])\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "        \n",
    "        dataset = TextDataset(sentences, labels, tokenizer, max_length)\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"数据加载错误: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_data_loader(dataset, batch_size, sampler_type='random'):\n",
    "    \"\"\"创建数据加载器\"\"\"\n",
    "    if sampler_type == 'random':\n",
    "        sampler = torch.utils.data.RandomSampler(dataset)\n",
    "    else:\n",
    "        sampler = torch.utils.data.SequentialSampler(dataset)\n",
    "    \n",
    "    return DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "def calculate_metrics(preds, labels):\n",
    "    \"\"\"计算评估指标，不使用sklearn\"\"\"\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    tp = np.sum((pred_flat == 1) & (labels_flat == 1))\n",
    "    tn = np.sum((pred_flat == 0) & (labels_flat == 0))\n",
    "    fp = np.sum((pred_flat == 1) & (labels_flat == 0))\n",
    "    fn = np.sum((pred_flat == 0) & (labels_flat == 1))\n",
    "    \n",
    "    # 避免除零错误\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': (tp + tn) / len(labels_flat),\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    \"\"\"评估模型性能\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_input_mask = batch['attention_mask'].to(device)\n",
    "        b_labels = batch['labels'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        all_preds.append(logits)\n",
    "        all_labels.append(label_ids)\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "    return calculate_metrics(all_preds, all_labels)\n",
    "\n",
    "def predict_text(model, tokenizer, text, device, max_length=128):\n",
    "    \"\"\"预测单个文本的情感\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    inputs = tokenizer(text, truncation=True, padding='max_length', \n",
    "                      max_length=max_length, return_tensors='pt')\n",
    "    \n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "    confidence, predicted_class = torch.max(probs, dim=1)\n",
    "    \n",
    "    predicted_class = predicted_class.item()\n",
    "    confidence = confidence.item()\n",
    "    probs = probs.cpu().numpy()[0]\n",
    "    \n",
    "    return {\n",
    "        'predicted_class': predicted_class,\n",
    "        'confidence': confidence,\n",
    "        'probabilities': probs\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # 设置参数\n",
    "    MODEL_PATH = 'model_output/best_model2.pt'\n",
    "    TEST_FILE = 'test.csv'\n",
    "    MAX_LENGTH = 128\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        num_labels=6,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    \n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "        print(f\"模型权重已加载: {MODEL_PATH}\")\n",
    "    else:\n",
    "        print(f\"模型文件不存在: {MODEL_PATH}\")\n",
    "        return\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    test_dataset = load_data(TEST_FILE, tokenizer, MAX_LENGTH)\n",
    "    \n",
    "    if test_dataset is not None:\n",
    "        test_dataloader = create_data_loader(test_dataset, batch_size=16, sampler_type='sequential')\n",
    "        metrics = evaluate_model(model, test_dataloader, device)\n",
    "        \n",
    "        print(\"\\nEvaluation Results:\")\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score: {metrics['f1']:.4f}\")\n",
    "    \n",
    "    print(\"\\nExample Predictions:\")\n",
    "    test_texts = [\n",
    "        \"This is a great product!\",\n",
    "        \"I did not like the service at all.\",\n",
    "        \"The food was delicious and the staff were friendly.\",\n",
    "        \"I will never come back to this place again.\"\n",
    "    ]\n",
    "    \n",
    "    for text in test_texts:\n",
    "        result = predict_text(model, tokenizer, text, device, MAX_LENGTH)\n",
    "        print(f\"\\nText: {text}\")\n",
    "        print(f\"Predicted Class: {result['predicted_class']}\")\n",
    "        print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "        print(f\"Probabilities: {result['probabilities']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
