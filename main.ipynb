{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 966645,
     "status": "ok",
     "timestamp": 1747037874164,
     "user": {
      "displayName": "Yaojia Wang",
      "userId": "02583595600482354486"
     },
     "user_tz": -480
    },
    "id": "xx_5_nyja-45",
    "outputId": "285f9b1a-3848-4d88-9009-b5d0e30d18dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading data...\n",
      "文件 training.csv 中的标签分布:\n",
      "唯一标签值: [0 1 2 3 4 5]\n",
      "标签计数: {np.int64(0): 4665, np.int64(1): 5362, np.int64(2): 1304, np.int64(3): 2159, np.int64(4): 1937, np.int64(5): 572}\n",
      "文件 validation.csv 中的标签分布:\n",
      "唯一标签值: [0 1 2 3 4 5]\n",
      "标签计数: {np.int64(0): 549, np.int64(1): 704, np.int64(2): 178, np.int64(3): 275, np.int64(4): 212, np.int64(5): 81}\n",
      "检测到的最大标签值: 5，设置num_labels=6\n",
      "使用类别权重: tensor([0.5716, 0.4973, 2.0449, 1.2351, 1.3766, 4.6617])\n",
      "使用加权采样器处理类别不平衡\n",
      "Starting training...\n",
      "======== Epoch 1 / 15 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-34ea4852aab7>:195: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "<ipython-input-4-34ea4852aab7>:228: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 40 of 500. Elapsed: 0:00:03\n",
      "  Batch 80 of 500. Elapsed: 0:00:06\n",
      "  Batch 120 of 500. Elapsed: 0:00:10\n",
      "  Batch 160 of 500. Elapsed: 0:00:13\n",
      "  Batch 200 of 500. Elapsed: 0:00:16\n",
      "  Batch 240 of 500. Elapsed: 0:00:19\n",
      "  Batch 280 of 500. Elapsed: 0:00:22\n",
      "  Batch 320 of 500. Elapsed: 0:00:26\n",
      "  Batch 360 of 500. Elapsed: 0:00:29\n",
      "  Batch 400 of 500. Elapsed: 0:00:32\n",
      "  Batch 440 of 500. Elapsed: 0:00:35\n",
      "  Batch 480 of 500. Elapsed: 0:00:39\n",
      "  Average training loss: 1.4729\n",
      "  Training epoch took: 0:00:40\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.2101\n",
      "  Validation Loss: 1.5993\n",
      "  Precision: 0.1658\n",
      "  Recall: 0.4165\n",
      "  F1-Score: 0.2106\n",
      "  Validation took: 0:00:13\n",
      "  Best accuracy model saved at: model_output/best_model_acc.pt\n",
      "  Best loss model saved at: model_output/best_model.pt\n",
      "======== Epoch 2 / 15 ========\n",
      "Training...\n",
      "  Batch 40 of 500. Elapsed: 0:00:03\n",
      "  Batch 80 of 500. Elapsed: 0:00:07\n",
      "  Batch 120 of 500. Elapsed: 0:00:10\n",
      "  Batch 160 of 500. Elapsed: 0:00:13\n",
      "  Batch 200 of 500. Elapsed: 0:00:16\n",
      "  Batch 240 of 500. Elapsed: 0:00:20\n",
      "  Batch 280 of 500. Elapsed: 0:00:23\n",
      "  Batch 320 of 500. Elapsed: 0:00:26\n",
      "  Batch 360 of 500. Elapsed: 0:00:30\n",
      "  Batch 400 of 500. Elapsed: 0:00:33\n",
      "  Batch 440 of 500. Elapsed: 0:00:37\n",
      "  Batch 480 of 500. Elapsed: 0:00:40\n",
      "  Average training loss: 0.6764\n",
      "  Training epoch took: 0:00:42\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.6198\n",
      "  Validation Loss: 0.8090\n",
      "  Precision: 0.6108\n",
      "  Recall: 0.7269\n",
      "  F1-Score: 0.6107\n",
      "  Validation took: 0:00:13\n",
      "  Best accuracy model saved at: model_output/best_model_acc.pt\n",
      "  Best loss model saved at: model_output/best_model.pt\n",
      "======== Epoch 3 / 15 ========\n",
      "Training...\n",
      "  Batch 40 of 500. Elapsed: 0:00:03\n",
      "  Batch 80 of 500. Elapsed: 0:00:07\n",
      "  Batch 120 of 500. Elapsed: 0:00:10\n",
      "  Batch 160 of 500. Elapsed: 0:00:14\n",
      "  Batch 200 of 500. Elapsed: 0:00:17\n",
      "  Batch 240 of 500. Elapsed: 0:00:20\n",
      "  Batch 280 of 500. Elapsed: 0:00:24\n",
      "  Batch 320 of 500. Elapsed: 0:00:27\n",
      "  Batch 360 of 500. Elapsed: 0:00:31\n",
      "  Batch 400 of 500. Elapsed: 0:00:34\n",
      "  Batch 440 of 500. Elapsed: 0:00:38\n",
      "  Batch 480 of 500. Elapsed: 0:00:41\n",
      "  Average training loss: 0.4051\n",
      "  Training epoch took: 0:00:43\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.7039\n",
      "  Validation Loss: 0.6426\n",
      "  Precision: 0.6835\n",
      "  Recall: 0.7941\n",
      "  F1-Score: 0.6996\n",
      "  Validation took: 0:00:13\n",
      "  Best accuracy model saved at: model_output/best_model_acc.pt\n",
      "  Best loss model saved at: model_output/best_model.pt\n",
      "======== Epoch 4 / 15 ========\n",
      "Training...\n",
      "  Batch 40 of 500. Elapsed: 0:00:03\n",
      "  Batch 80 of 500. Elapsed: 0:00:07\n",
      "  Batch 120 of 500. Elapsed: 0:00:10\n",
      "  Batch 160 of 500. Elapsed: 0:00:13\n",
      "  Batch 200 of 500. Elapsed: 0:00:17\n",
      "  Batch 240 of 500. Elapsed: 0:00:20\n",
      "  Batch 280 of 500. Elapsed: 0:00:24\n",
      "  Batch 320 of 500. Elapsed: 0:00:27\n",
      "  Batch 360 of 500. Elapsed: 0:00:31\n",
      "  Batch 400 of 500. Elapsed: 0:00:34\n",
      "  Batch 440 of 500. Elapsed: 0:00:38\n",
      "  Batch 480 of 500. Elapsed: 0:00:42\n",
      "  Average training loss: 0.3029\n",
      "  Training epoch took: 0:00:43\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.7479\n",
      "  Validation Loss: 0.5666\n",
      "  Precision: 0.7225\n",
      "  Recall: 0.8215\n",
      "  F1-Score: 0.7425\n",
      "  Validation took: 0:00:13\n",
      "  Best accuracy model saved at: model_output/best_model_acc.pt\n",
      "  Best loss model saved at: model_output/best_model.pt\n",
      "======== Epoch 5 / 15 ========\n",
      "Training...\n",
      "  Batch 40 of 500. Elapsed: 0:00:03\n",
      "  Batch 80 of 500. Elapsed: 0:00:07\n",
      "  Batch 120 of 500. Elapsed: 0:00:10\n",
      "  Batch 160 of 500. Elapsed: 0:00:13\n",
      "  Batch 200 of 500. Elapsed: 0:00:17\n",
      "  Batch 240 of 500. Elapsed: 0:00:20\n",
      "  Batch 280 of 500. Elapsed: 0:00:24\n",
      "  Batch 320 of 500. Elapsed: 0:00:27\n",
      "  Batch 360 of 500. Elapsed: 0:00:30\n",
      "  Batch 400 of 500. Elapsed: 0:00:34\n",
      "  Batch 440 of 500. Elapsed: 0:00:38\n",
      "  Batch 480 of 500. Elapsed: 0:00:41\n",
      "  Average training loss: 0.2624\n",
      "  Training epoch took: 0:00:43\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.7994\n",
      "  Validation Loss: 0.4933\n",
      "  Precision: 0.7539\n",
      "  Recall: 0.8433\n",
      "  F1-Score: 0.7837\n",
      "  Validation took: 0:00:14\n",
      "  Best accuracy model saved at: model_output/best_model_acc.pt\n",
      "  Best loss model saved at: model_output/best_model.pt\n",
      "======== Epoch 6 / 15 ========\n",
      "Training...\n",
      "  Batch 40 of 500. Elapsed: 0:00:03\n",
      "  Batch 80 of 500. Elapsed: 0:00:07\n",
      "  Batch 120 of 500. Elapsed: 0:00:10\n",
      "  Batch 160 of 500. Elapsed: 0:00:13\n",
      "  Batch 200 of 500. Elapsed: 0:00:17\n",
      "  Batch 240 of 500. Elapsed: 0:00:20\n",
      "  Batch 280 of 500. Elapsed: 0:00:24\n",
      "  Batch 320 of 500. Elapsed: 0:00:27\n",
      "  Batch 360 of 500. Elapsed: 0:00:30\n",
      "  Batch 400 of 500. Elapsed: 0:00:34\n",
      "  Batch 440 of 500. Elapsed: 0:00:37\n",
      "  Batch 480 of 500. Elapsed: 0:00:41\n",
      "  Average training loss: 0.2343\n",
      "  Training epoch took: 0:00:43\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.8254\n",
      "  Validation Loss: 0.4600\n",
      "  Precision: 0.7789\n",
      "  Recall: 0.8630\n",
      "  F1-Score: 0.8087\n",
      "  Validation took: 0:00:13\n",
      "  Best accuracy model saved at: model_output/best_model_acc.pt\n",
      "  Best loss model saved at: model_output/best_model.pt\n",
      "======== Epoch 7 / 15 ========\n",
      "Training...\n",
      "  Batch 40 of 500. Elapsed: 0:00:03\n",
      "  Batch 80 of 500. Elapsed: 0:00:07\n",
      "  Batch 120 of 500. Elapsed: 0:00:10\n",
      "  Batch 160 of 500. Elapsed: 0:00:13\n",
      "  Batch 200 of 500. Elapsed: 0:00:17\n",
      "  Batch 240 of 500. Elapsed: 0:00:20\n",
      "  Batch 280 of 500. Elapsed: 0:00:24\n",
      "  Batch 320 of 500. Elapsed: 0:00:27\n",
      "  Batch 360 of 500. Elapsed: 0:00:31\n",
      "  Batch 400 of 500. Elapsed: 0:00:34\n",
      "  Batch 440 of 500. Elapsed: 0:00:38\n",
      "  Batch 480 of 500. Elapsed: 0:00:42\n",
      "  Average training loss: 0.2014\n",
      "  Training epoch took: 0:00:43\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.8169\n",
      "  Validation Loss: 0.4682\n",
      "  Precision: 0.7741\n",
      "  Recall: 0.8649\n",
      "  F1-Score: 0.8032\n",
      "  Validation took: 0:00:13\n",
      "======== Epoch 8 / 15 ========\n",
      "Training...\n",
      "  Batch 40 of 500. Elapsed: 0:00:03\n",
      "  Batch 80 of 500. Elapsed: 0:00:07\n",
      "  Batch 120 of 500. Elapsed: 0:00:10\n",
      "  Batch 160 of 500. Elapsed: 0:00:13\n",
      "  Batch 200 of 500. Elapsed: 0:00:17\n",
      "  Batch 240 of 500. Elapsed: 0:00:20\n",
      "  Batch 280 of 500. Elapsed: 0:00:24\n",
      "  Batch 320 of 500. Elapsed: 0:00:27\n",
      "  Batch 360 of 500. Elapsed: 0:00:30\n",
      "  Batch 400 of 500. Elapsed: 0:00:34\n",
      "  Batch 440 of 500. Elapsed: 0:00:37\n",
      "  Batch 480 of 500. Elapsed: 0:00:40\n",
      "  Average training loss: 0.1756\n",
      "  Training epoch took: 0:00:42\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.8404\n",
      "  Validation Loss: 0.4519\n",
      "  Precision: 0.7919\n",
      "  Recall: 0.8729\n",
      "  F1-Score: 0.8225\n",
      "  Validation took: 0:00:13\n",
      "  Best accuracy model saved at: model_output/best_model_acc.pt\n",
      "  Best loss model saved at: model_output/best_model.pt\n",
      "======== Epoch 9 / 15 ========\n",
      "Training...\n",
      "  Batch 40 of 500. Elapsed: 0:00:03\n",
      "  Batch 80 of 500. Elapsed: 0:00:07\n",
      "  Batch 120 of 500. Elapsed: 0:00:10\n",
      "  Batch 160 of 500. Elapsed: 0:00:14\n",
      "  Batch 200 of 500. Elapsed: 0:00:17\n",
      "  Batch 240 of 500. Elapsed: 0:00:21\n",
      "  Batch 280 of 500. Elapsed: 0:00:24\n",
      "  Batch 320 of 500. Elapsed: 0:00:28\n",
      "  Batch 360 of 500. Elapsed: 0:00:31\n",
      "  Batch 400 of 500. Elapsed: 0:00:35\n",
      "  Batch 440 of 500. Elapsed: 0:00:38\n",
      "  Batch 480 of 500. Elapsed: 0:00:42\n",
      "  Average training loss: 0.1743\n",
      "  Training epoch took: 0:00:44\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.8534\n",
      "  Validation Loss: 0.4195\n",
      "  Precision: 0.8070\n",
      "  Recall: 0.8793\n",
      "  F1-Score: 0.8355\n",
      "  Validation took: 0:00:13\n",
      "  Best accuracy model saved at: model_output/best_model_acc.pt\n",
      "  Best loss model saved at: model_output/best_model.pt\n",
      "======== Epoch 10 / 15 ========\n",
      "Training...\n",
      "  Batch 40 of 500. Elapsed: 0:00:03\n",
      "  Batch 80 of 500. Elapsed: 0:00:07\n",
      "  Batch 120 of 500. Elapsed: 0:00:10\n",
      "  Batch 160 of 500. Elapsed: 0:00:13\n",
      "  Batch 200 of 500. Elapsed: 0:00:17\n",
      "  Batch 240 of 500. Elapsed: 0:00:20\n",
      "  Batch 280 of 500. Elapsed: 0:00:24\n",
      "  Batch 320 of 500. Elapsed: 0:00:27\n",
      "  Batch 360 of 500. Elapsed: 0:00:30\n",
      "  Batch 400 of 500. Elapsed: 0:00:34\n",
      "  Batch 440 of 500. Elapsed: 0:00:38\n",
      "  Batch 480 of 500. Elapsed: 0:00:41\n",
      "  Average training loss: 0.1582\n",
      "  Training epoch took: 0:00:43\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.8499\n",
      "  Validation Loss: 0.4335\n",
      "  Precision: 0.8038\n",
      "  Recall: 0.8822\n",
      "  F1-Score: 0.8340\n",
      "  Validation took: 0:00:14\n",
      "======== Epoch 11 / 15 ========\n",
      "Training...\n",
      "  Batch 40 of 500. Elapsed: 0:00:03\n",
      "  Batch 80 of 500. Elapsed: 0:00:07\n",
      "  Batch 120 of 500. Elapsed: 0:00:10\n",
      "  Batch 160 of 500. Elapsed: 0:00:14\n",
      "  Batch 200 of 500. Elapsed: 0:00:17\n",
      "  Batch 240 of 500. Elapsed: 0:00:20\n",
      "  Batch 280 of 500. Elapsed: 0:00:24\n",
      "  Batch 320 of 500. Elapsed: 0:00:27\n",
      "  Batch 360 of 500. Elapsed: 0:00:30\n",
      "  Batch 400 of 500. Elapsed: 0:00:34\n",
      "  Batch 440 of 500. Elapsed: 0:00:37\n",
      "  Batch 480 of 500. Elapsed: 0:00:40\n",
      "  Average training loss: 0.1501\n",
      "  Training epoch took: 0:00:42\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.8494\n",
      "  Validation Loss: 0.4293\n",
      "  Precision: 0.8015\n",
      "  Recall: 0.8816\n",
      "  F1-Score: 0.8310\n",
      "  Validation took: 0:00:13\n",
      "======== Epoch 12 / 15 ========\n",
      "Training...\n",
      "  Batch 40 of 500. Elapsed: 0:00:03\n",
      "  Batch 80 of 500. Elapsed: 0:00:07\n",
      "  Batch 120 of 500. Elapsed: 0:00:10\n",
      "  Batch 160 of 500. Elapsed: 0:00:14\n",
      "  Batch 200 of 500. Elapsed: 0:00:17\n",
      "  Batch 240 of 500. Elapsed: 0:00:21\n",
      "  Batch 280 of 500. Elapsed: 0:00:24\n",
      "  Batch 320 of 500. Elapsed: 0:00:27\n",
      "  Batch 360 of 500. Elapsed: 0:00:31\n",
      "  Batch 400 of 500. Elapsed: 0:00:34\n",
      "  Batch 440 of 500. Elapsed: 0:00:38\n",
      "  Batch 480 of 500. Elapsed: 0:00:41\n",
      "  Average training loss: 0.1453\n",
      "  Training epoch took: 0:00:43\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.8549\n",
      "  Validation Loss: 0.4215\n",
      "  Precision: 0.8096\n",
      "  Recall: 0.8841\n",
      "  F1-Score: 0.8379\n",
      "  Validation took: 0:00:13\n",
      "  Best accuracy model saved at: model_output/best_model_acc.pt\n",
      "======== Epoch 13 / 15 ========\n",
      "Training...\n",
      "  Batch 40 of 500. Elapsed: 0:00:03\n",
      "  Batch 80 of 500. Elapsed: 0:00:07\n",
      "  Batch 120 of 500. Elapsed: 0:00:10\n",
      "  Batch 160 of 500. Elapsed: 0:00:14\n",
      "  Batch 200 of 500. Elapsed: 0:00:17\n",
      "  Batch 240 of 500. Elapsed: 0:00:20\n",
      "  Batch 280 of 500. Elapsed: 0:00:24\n",
      "  Batch 320 of 500. Elapsed: 0:00:27\n",
      "  Batch 360 of 500. Elapsed: 0:00:31\n",
      "  Batch 400 of 500. Elapsed: 0:00:34\n",
      "  Batch 440 of 500. Elapsed: 0:00:38\n",
      "  Batch 480 of 500. Elapsed: 0:00:41\n",
      "  Average training loss: 0.1380\n",
      "  Training epoch took: 0:00:43\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.8569\n",
      "  Validation Loss: 0.4163\n",
      "  Precision: 0.8116\n",
      "  Recall: 0.8848\n",
      "  F1-Score: 0.8394\n",
      "  Validation took: 0:00:13\n",
      "  Best accuracy model saved at: model_output/best_model_acc.pt\n",
      "  Best loss model saved at: model_output/best_model.pt\n",
      "======== Epoch 14 / 15 ========\n",
      "Training...\n",
      "  Batch 40 of 500. Elapsed: 0:00:03\n",
      "  Batch 80 of 500. Elapsed: 0:00:07\n",
      "  Batch 120 of 500. Elapsed: 0:00:10\n",
      "  Batch 160 of 500. Elapsed: 0:00:13\n",
      "  Batch 200 of 500. Elapsed: 0:00:17\n",
      "  Batch 240 of 500. Elapsed: 0:00:20\n",
      "  Batch 280 of 500. Elapsed: 0:00:24\n",
      "  Batch 320 of 500. Elapsed: 0:00:27\n",
      "  Batch 360 of 500. Elapsed: 0:00:30\n",
      "  Batch 400 of 500. Elapsed: 0:00:34\n",
      "  Batch 440 of 500. Elapsed: 0:00:37\n",
      "  Batch 480 of 500. Elapsed: 0:00:41\n",
      "  Average training loss: 0.1342\n",
      "  Training epoch took: 0:00:42\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.8594\n",
      "  Validation Loss: 0.4218\n",
      "  Precision: 0.8139\n",
      "  Recall: 0.8858\n",
      "  F1-Score: 0.8419\n",
      "  Validation took: 0:00:13\n",
      "  Best accuracy model saved at: model_output/best_model_acc.pt\n",
      "======== Epoch 15 / 15 ========\n",
      "Training...\n",
      "  Batch 40 of 500. Elapsed: 0:00:03\n",
      "  Batch 80 of 500. Elapsed: 0:00:07\n",
      "  Batch 120 of 500. Elapsed: 0:00:10\n",
      "  Batch 160 of 500. Elapsed: 0:00:14\n",
      "  Batch 200 of 500. Elapsed: 0:00:17\n",
      "  Batch 240 of 500. Elapsed: 0:00:20\n",
      "  Batch 280 of 500. Elapsed: 0:00:24\n",
      "  Batch 320 of 500. Elapsed: 0:00:27\n",
      "  Batch 360 of 500. Elapsed: 0:00:31\n",
      "  Batch 400 of 500. Elapsed: 0:00:34\n",
      "  Batch 440 of 500. Elapsed: 0:00:38\n",
      "  Batch 480 of 500. Elapsed: 0:00:41\n",
      "  Average training loss: 0.1355\n",
      "  Training epoch took: 0:00:43\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.8609\n",
      "  Validation Loss: 0.4204\n",
      "  Precision: 0.8151\n",
      "  Recall: 0.8869\n",
      "  F1-Score: 0.8432\n",
      "  Validation took: 0:00:13\n",
      "  Best accuracy model saved at: model_output/best_model_acc.pt\n",
      "\n",
      "Training complete! Total training took 0:15:56\n",
      "Best validation accuracy: 0.8609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-34ea4852aab7>:386: UserWarning: Glyph 35757 (\\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'loss_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:386: UserWarning: Glyph 32451 (\\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'loss_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:386: UserWarning: Glyph 21644 (\\N{CJK UNIFIED IDEOGRAPH-548C}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'loss_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:386: UserWarning: Glyph 39564 (\\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'loss_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:386: UserWarning: Glyph 35777 (\\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'loss_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:386: UserWarning: Glyph 25439 (\\N{CJK UNIFIED IDEOGRAPH-635F}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'loss_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:386: UserWarning: Glyph 22833 (\\N{CJK UNIFIED IDEOGRAPH-5931}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'loss_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:397: UserWarning: Glyph 39564 (\\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'accuracy_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:397: UserWarning: Glyph 35777 (\\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'accuracy_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:397: UserWarning: Glyph 20934 (\\N{CJK UNIFIED IDEOGRAPH-51C6}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'accuracy_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:397: UserWarning: Glyph 30830 (\\N{CJK UNIFIED IDEOGRAPH-786E}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'accuracy_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:397: UserWarning: Glyph 29575 (\\N{CJK UNIFIED IDEOGRAPH-7387}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'accuracy_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:410: UserWarning: Glyph 39564 (\\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'metrics_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:410: UserWarning: Glyph 35777 (\\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'metrics_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:410: UserWarning: Glyph 38598 (\\N{CJK UNIFIED IDEOGRAPH-96C6}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'metrics_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:410: UserWarning: Glyph 35780 (\\N{CJK UNIFIED IDEOGRAPH-8BC4}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'metrics_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:410: UserWarning: Glyph 20272 (\\N{CJK UNIFIED IDEOGRAPH-4F30}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'metrics_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:410: UserWarning: Glyph 25351 (\\N{CJK UNIFIED IDEOGRAPH-6307}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'metrics_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:410: UserWarning: Glyph 26631 (\\N{CJK UNIFIED IDEOGRAPH-6807}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'metrics_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:410: UserWarning: Glyph 31934 (\\N{CJK UNIFIED IDEOGRAPH-7CBE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'metrics_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:410: UserWarning: Glyph 30830 (\\N{CJK UNIFIED IDEOGRAPH-786E}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'metrics_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:410: UserWarning: Glyph 29575 (\\N{CJK UNIFIED IDEOGRAPH-7387}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'metrics_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:410: UserWarning: Glyph 21484 (\\N{CJK UNIFIED IDEOGRAPH-53EC}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'metrics_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:410: UserWarning: Glyph 22238 (\\N{CJK UNIFIED IDEOGRAPH-56DE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'metrics_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:410: UserWarning: Glyph 20998 (\\N{CJK UNIFIED IDEOGRAPH-5206}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'metrics_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:410: UserWarning: Glyph 25968 (\\N{CJK UNIFIED IDEOGRAPH-6570}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'metrics_curve.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:434: UserWarning: Glyph 35757 (\\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "<ipython-input-4-34ea4852aab7>:434: UserWarning: Glyph 32451 (\\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "<ipython-input-4-34ea4852aab7>:434: UserWarning: Glyph 21644 (\\N{CJK UNIFIED IDEOGRAPH-548C}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "<ipython-input-4-34ea4852aab7>:434: UserWarning: Glyph 39564 (\\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "<ipython-input-4-34ea4852aab7>:434: UserWarning: Glyph 35777 (\\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "<ipython-input-4-34ea4852aab7>:434: UserWarning: Glyph 25439 (\\N{CJK UNIFIED IDEOGRAPH-635F}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "<ipython-input-4-34ea4852aab7>:434: UserWarning: Glyph 22833 (\\N{CJK UNIFIED IDEOGRAPH-5931}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "<ipython-input-4-34ea4852aab7>:434: UserWarning: Glyph 38598 (\\N{CJK UNIFIED IDEOGRAPH-96C6}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "<ipython-input-4-34ea4852aab7>:434: UserWarning: Glyph 35780 (\\N{CJK UNIFIED IDEOGRAPH-8BC4}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "<ipython-input-4-34ea4852aab7>:434: UserWarning: Glyph 20272 (\\N{CJK UNIFIED IDEOGRAPH-4F30}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "<ipython-input-4-34ea4852aab7>:434: UserWarning: Glyph 25351 (\\N{CJK UNIFIED IDEOGRAPH-6307}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "<ipython-input-4-34ea4852aab7>:434: UserWarning: Glyph 26631 (\\N{CJK UNIFIED IDEOGRAPH-6807}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "<ipython-input-4-34ea4852aab7>:434: UserWarning: Glyph 20934 (\\N{CJK UNIFIED IDEOGRAPH-51C6}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "<ipython-input-4-34ea4852aab7>:434: UserWarning: Glyph 30830 (\\N{CJK UNIFIED IDEOGRAPH-786E}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "<ipython-input-4-34ea4852aab7>:434: UserWarning: Glyph 29575 (\\N{CJK UNIFIED IDEOGRAPH-7387}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "<ipython-input-4-34ea4852aab7>:434: UserWarning: Glyph 31934 (\\N{CJK UNIFIED IDEOGRAPH-7CBE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "<ipython-input-4-34ea4852aab7>:434: UserWarning: Glyph 21484 (\\N{CJK UNIFIED IDEOGRAPH-53EC}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "<ipython-input-4-34ea4852aab7>:434: UserWarning: Glyph 22238 (\\N{CJK UNIFIED IDEOGRAPH-56DE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "<ipython-input-4-34ea4852aab7>:434: UserWarning: Glyph 20998 (\\N{CJK UNIFIED IDEOGRAPH-5206}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "<ipython-input-4-34ea4852aab7>:434: UserWarning: Glyph 25968 (\\N{CJK UNIFIED IDEOGRAPH-6570}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "<ipython-input-4-34ea4852aab7>:435: UserWarning: Glyph 35757 (\\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'all_metrics.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:435: UserWarning: Glyph 32451 (\\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'all_metrics.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:435: UserWarning: Glyph 21644 (\\N{CJK UNIFIED IDEOGRAPH-548C}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'all_metrics.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:435: UserWarning: Glyph 39564 (\\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'all_metrics.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:435: UserWarning: Glyph 35777 (\\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'all_metrics.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:435: UserWarning: Glyph 25439 (\\N{CJK UNIFIED IDEOGRAPH-635F}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'all_metrics.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:435: UserWarning: Glyph 22833 (\\N{CJK UNIFIED IDEOGRAPH-5931}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'all_metrics.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:435: UserWarning: Glyph 38598 (\\N{CJK UNIFIED IDEOGRAPH-96C6}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'all_metrics.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:435: UserWarning: Glyph 35780 (\\N{CJK UNIFIED IDEOGRAPH-8BC4}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'all_metrics.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:435: UserWarning: Glyph 20272 (\\N{CJK UNIFIED IDEOGRAPH-4F30}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'all_metrics.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:435: UserWarning: Glyph 25351 (\\N{CJK UNIFIED IDEOGRAPH-6307}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'all_metrics.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:435: UserWarning: Glyph 26631 (\\N{CJK UNIFIED IDEOGRAPH-6807}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'all_metrics.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:435: UserWarning: Glyph 20934 (\\N{CJK UNIFIED IDEOGRAPH-51C6}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'all_metrics.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:435: UserWarning: Glyph 30830 (\\N{CJK UNIFIED IDEOGRAPH-786E}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'all_metrics.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:435: UserWarning: Glyph 29575 (\\N{CJK UNIFIED IDEOGRAPH-7387}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'all_metrics.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:435: UserWarning: Glyph 31934 (\\N{CJK UNIFIED IDEOGRAPH-7CBE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'all_metrics.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:435: UserWarning: Glyph 21484 (\\N{CJK UNIFIED IDEOGRAPH-53EC}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'all_metrics.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:435: UserWarning: Glyph 22238 (\\N{CJK UNIFIED IDEOGRAPH-56DE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'all_metrics.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:435: UserWarning: Glyph 20998 (\\N{CJK UNIFIED IDEOGRAPH-5206}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'all_metrics.pdf'), dpi=300, bbox_inches='tight')\n",
      "<ipython-input-4-34ea4852aab7>:435: UserWarning: Glyph 25968 (\\N{CJK UNIFIED IDEOGRAPH-6570}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(plots_dir, 'all_metrics.pdf'), dpi=300, bbox_inches='tight')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练历史图表已保存到: model_output/plots\n",
      "训练历史数据已保存到: model_output/training_history.csv\n",
      "Training completed with best validation loss: 0.4163\n",
      "Best validation accuracy: 0.8609\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup, BertModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(sentences, truncation=True, padding='max_length', max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"设置随机种子，确保结果可复现\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def load_data(file_path, tokenizer, max_length=128):\n",
    "    \"\"\"加载并处理数据，不使用pandas\"\"\"\n",
    "    try:\n",
    "        sentences = []\n",
    "        labels = []\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            # 跳过标题行\n",
    "            next(f)\n",
    "\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "\n",
    "                # 假设CSV格式为: sentence,label\n",
    "                parts = line.split(',')\n",
    "                if len(parts) >= 2:\n",
    "                    sentence = ','.join(parts[:-1])  # 处理句子中可能包含的逗号\n",
    "                    label = int(parts[-1])\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "\n",
    "        dataset = TextDataset(sentences, labels, tokenizer, max_length)\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"数据加载错误: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_weighted_sampler(dataset):\n",
    "    \"\"\"创建加权采样器以处理类别不平衡\"\"\"\n",
    "    labels = np.array(dataset.labels)\n",
    "    class_counts = np.bincount(labels)\n",
    "    class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
    "    sample_weights = class_weights[labels]\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    return sampler\n",
    "\n",
    "def create_data_loader(dataset, batch_size, sampler_type='random'):\n",
    "    \"\"\"创建数据加载器\"\"\"\n",
    "    if sampler_type == 'weighted':\n",
    "        sampler = create_weighted_sampler(dataset)\n",
    "        return DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n",
    "    elif sampler_type == 'random':\n",
    "        sampler = torch.utils.data.RandomSampler(dataset)\n",
    "    else:\n",
    "        sampler = torch.utils.data.SequentialSampler(dataset)\n",
    "\n",
    "    return DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "def calculate_metrics_multiclass_improved(preds, labels):\n",
    "    \"\"\"计算多分类任务的评估指标\"\"\"\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    # 计算每个类别的精确率、召回率和F1分数\n",
    "    metrics = {'accuracy': np.sum(pred_flat == labels_flat) / len(labels_flat)}\n",
    "\n",
    "    classes = np.unique(np.concatenate([pred_flat, labels_flat]))\n",
    "\n",
    "    # 计算宏平均指标\n",
    "    precision_sum = 0\n",
    "    recall_sum = 0\n",
    "    f1_sum = 0\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    for cls in classes:\n",
    "        tp = np.sum((pred_flat == cls) & (labels_flat == cls))\n",
    "        fp = np.sum((pred_flat == cls) & (labels_flat != cls))\n",
    "        fn = np.sum((pred_flat != cls) & (labels_flat == cls))\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        precision_sum += precision\n",
    "        recall_sum += recall\n",
    "        f1_sum += f1\n",
    "\n",
    "    metrics['precision'] = precision_sum / n_classes\n",
    "    metrics['recall'] = recall_sum / n_classes\n",
    "    metrics['f1'] = f1_sum / n_classes\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# 增强的BERT分类模型\n",
    "class EnhancedBertForSequenceClassification(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_labels):\n",
    "        super(EnhancedBertForSequenceClassification, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.num_labels = num_labels  # 存储标签数量\n",
    "\n",
    "        # 冻结BERT底层参数以专注于微调顶层\n",
    "        for param in list(self.bert.parameters())[:-4*12]:  # 冻结除最后4层外的所有层\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # 丰富的分类头部\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, 768),\n",
    "            nn.LayerNorm(768),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(768, 384),\n",
    "            nn.LayerNorm(384),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(384, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        pooled_output = outputs.pooler_output\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # 计算每个类别的权重，反比于其频率\n",
    "            if hasattr(self, 'class_weights') and self.class_weights is not None:\n",
    "                loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(labels.device))\n",
    "            else:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def set_class_weights(self, class_weights):\n",
    "        \"\"\"设置类别权重用于损失函数\"\"\"\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, optimizer, scheduler, device, epochs, save_path):\n",
    "    \"\"\"训练模型\"\"\"\n",
    "    total_t0 = time.time()\n",
    "    best_val_loss = float('inf')\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    # 添加早停变量\n",
    "    patience = 3\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    # 创建梯度缩放器用于混合精度训练\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # 记录训练过程中的指标\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': []\n",
    "    }\n",
    "\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print(f'======== Epoch {epoch_i + 1} / {epochs} ========')\n",
    "        print('Training...')\n",
    "\n",
    "        t0 = time.time()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print(f'  Batch {step} of {len(train_dataloader)}. Elapsed: {elapsed}')\n",
    "\n",
    "            b_input_ids = batch['input_ids'].to(device)\n",
    "            b_input_mask = batch['attention_mask'].to(device)\n",
    "            b_labels = batch['labels'].to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            # 使用混合精度训练\n",
    "            with autocast():\n",
    "                outputs = model(b_input_ids,\n",
    "                                token_type_ids=None,\n",
    "                                attention_mask=b_input_mask,\n",
    "                                labels=b_labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # 缩放梯度并反向传播\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(f\"  Average training loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Training epoch took: {training_time}\")\n",
    "\n",
    "        print(\"\\nRunning Validation...\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total_eval_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for batch in val_dataloader:\n",
    "            b_input_ids = batch['input_ids'].to(device)\n",
    "            b_input_mask = batch['attention_mask'].to(device)\n",
    "            b_labels = batch['labels'].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids,\n",
    "                                token_type_ids=None,\n",
    "                                attention_mask=b_input_mask,\n",
    "                                labels=b_labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            all_preds.append(logits)\n",
    "            all_labels.append(label_ids)\n",
    "\n",
    "        all_preds = np.concatenate(all_preds, axis=0)\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "        avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "        metrics = calculate_metrics_multiclass_improved(all_preds, all_labels)\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "\n",
    "        # 记录当前epoch的指标\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['accuracy'].append(metrics['accuracy'])\n",
    "        history['precision'].append(metrics['precision'])\n",
    "        history['recall'].append(metrics['recall'])\n",
    "        history['f1'].append(metrics['f1'])\n",
    "\n",
    "        print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1-Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"  Validation took: {validation_time}\")\n",
    "\n",
    "        # 验证后的早停检查 - 根据准确率和损失共同判断\n",
    "        current_accuracy = metrics['accuracy']\n",
    "        if current_accuracy > best_accuracy:\n",
    "            best_accuracy = current_accuracy\n",
    "            early_stop_counter = 0\n",
    "            model_path = os.path.join(save_path, 'best_model_acc.pt')\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"  Best accuracy model saved at: {model_path}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            if abs(best_accuracy - current_accuracy) <= 0.01:  # 如果精度相近，优先选择低损失模型\n",
    "                early_stop_counter = 0\n",
    "                model_path = os.path.join(save_path, 'best_model.pt')\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print(f\"  Best loss model saved at: {model_path}\")\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(f\"Early stopping after {epoch_i + 1} epochs\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\nTraining complete! Total training took {format_time(time.time()-total_t0)}\")\n",
    "    print(f\"Best validation accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "    # 保存并绘制训练历史\n",
    "    plot_training_history(history, save_path)\n",
    "\n",
    "    return best_val_loss, best_accuracy, history\n",
    "\n",
    "def format_time(elapsed):\n",
    "    \"\"\"将时间格式化为 hh:mm:ss\"\"\"\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def inspect_dataset(file_path):\n",
    "    \"\"\"检查数据集中的标签分布\"\"\"\n",
    "    labels = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        next(f)  # 跳过标题行\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            if len(parts) >= 2:\n",
    "                label = int(parts[-1])\n",
    "                labels.append(label)\n",
    "\n",
    "    unique_labels = np.unique(labels)\n",
    "    counts = {label: labels.count(label) for label in unique_labels}\n",
    "\n",
    "    print(f\"文件 {file_path} 中的标签分布:\")\n",
    "    print(f\"唯一标签值: {unique_labels}\")\n",
    "    print(f\"标签计数: {counts}\")\n",
    "\n",
    "    return unique_labels, counts\n",
    "\n",
    "def calculate_class_weights(counts, num_labels):\n",
    "    \"\"\"计算类别权重\"\"\"\n",
    "    weights = torch.zeros(num_labels)\n",
    "    total_samples = sum(counts.values())\n",
    "\n",
    "    for label, count in counts.items():\n",
    "        weights[label] = total_samples / (count * num_labels)\n",
    "\n",
    "    return weights\n",
    "\n",
    "def plot_training_history(history, save_path):\n",
    "    \"\"\"Plot training metrics curves\"\"\"\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    # Create directory for plots if it doesn't exist\n",
    "    plots_dir = os.path.join(save_path, 'plots')\n",
    "    if not os.path.exists(plots_dir):\n",
    "        os.makedirs(plots_dir)\n",
    "\n",
    "    # 1. Plot loss curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n",
    "    plt.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(plots_dir, 'loss_curve.pdf'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Plot accuracy curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, history['accuracy'], 'g-', label='Accuracy')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(plots_dir, 'accuracy_curve.pdf'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Plot precision, recall, and F1 score curves\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(epochs, history['precision'], 'b-', label='Precision')\n",
    "    plt.plot(epochs, history['recall'], 'r-', label='Recall')\n",
    "    plt.plot(epochs, history['f1'], 'g-', label='F1-Score')\n",
    "    plt.title('Validation Metrics')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(plots_dir, 'metrics_curve.pdf'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # 4. Plot all metrics in one figure\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n",
    "    plt.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(epochs, history['accuracy'], 'm-', label='Accuracy')\n",
    "    plt.plot(epochs, history['precision'], 'c-', label='Precision')\n",
    "    plt.plot(epochs, history['recall'], 'y-', label='Recall')\n",
    "    plt.plot(epochs, history['f1'], 'g-', label='F1-Score')\n",
    "    plt.title('Validation Metrics')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, 'all_metrics.pdf'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"训练历史图表已保存到: {plots_dir}\")\n",
    "\n",
    "    # 保存历史数据为CSV文件\n",
    "    import csv\n",
    "    csv_path = os.path.join(save_path, 'training_history.csv')\n",
    "    with open(csv_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['epoch', 'train_loss', 'val_loss', 'accuracy', 'precision', 'recall', 'f1'])\n",
    "        for i in range(len(epochs)):\n",
    "            writer.writerow([epochs[i],\n",
    "                            history['train_loss'][i],\n",
    "                            history['val_loss'][i],\n",
    "                            history['accuracy'][i],\n",
    "                            history['precision'][i],\n",
    "                            history['recall'][i],\n",
    "                            history['f1'][i]])\n",
    "    print(f\"训练历史数据已保存到: {csv_path}\")\n",
    "\n",
    "def main():\n",
    "    # 设置参数\n",
    "    SEED = 42\n",
    "    BATCH_SIZE = 32  # 增大批量大小\n",
    "    LEARNING_RATE = 1e-5  # 使用更合适的学习率\n",
    "    EPSILON = 1e-8\n",
    "    EPOCHS = 15  # 增加训练轮次以便更好地学习\n",
    "    MAX_LENGTH = 128\n",
    "    SAVE_PATH = 'model_output'\n",
    "    TRAIN_FILE = 'training.csv'\n",
    "    VAL_FILE = 'validation.csv'\n",
    "    USE_CLASS_WEIGHTS = True  # 使用类别权重\n",
    "    USE_WEIGHTED_SAMPLER = True  # 使用加权采样器\n",
    "\n",
    "    set_seed(SEED)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    train_dataset = load_data(TRAIN_FILE, tokenizer, MAX_LENGTH)\n",
    "    val_dataset = load_data(VAL_FILE, tokenizer, MAX_LENGTH)\n",
    "\n",
    "    if train_dataset is None or val_dataset is None:\n",
    "        print(\"数据加载失败，程序退出\")\n",
    "        return\n",
    "\n",
    "    train_labels, train_counts = inspect_dataset(TRAIN_FILE)\n",
    "    val_labels, val_counts = inspect_dataset(VAL_FILE)\n",
    "\n",
    "    # 确保模型配置与数据匹配\n",
    "    num_labels = max(max(train_labels), max(val_labels)) + 1\n",
    "    print(f\"检测到的最大标签值: {num_labels-1}，设置num_labels={num_labels}\")\n",
    "\n",
    "    # 使用增强的BERT分类模型\n",
    "    model = EnhancedBertForSequenceClassification('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "    # 计算并设置类别权重\n",
    "    if USE_CLASS_WEIGHTS:\n",
    "        class_weights = calculate_class_weights(train_counts, num_labels)\n",
    "        model.set_class_weights(class_weights)\n",
    "        print(f\"使用类别权重: {class_weights}\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # 使用加权采样器处理类别不平衡\n",
    "    if USE_WEIGHTED_SAMPLER:\n",
    "        train_dataloader = create_data_loader(train_dataset, BATCH_SIZE, 'weighted')\n",
    "        print(\"使用加权采样器处理类别不平衡\")\n",
    "    else:\n",
    "        train_dataloader = create_data_loader(train_dataset, BATCH_SIZE, 'random')\n",
    "\n",
    "    val_dataloader = create_data_loader(val_dataset, BATCH_SIZE, 'sequential')\n",
    "\n",
    "    # 使用较低的初始学习率，增加权重衰减\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=EPSILON, weight_decay=0.01)\n",
    "\n",
    "    total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "    # 修改学习率调度策略，添加预热步骤\n",
    "    warmup_steps = int(0.1 * total_steps)  # 10%的预热步骤\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    best_val_loss, best_accuracy, history = train_model(model, train_dataloader, val_dataloader, optimizer, scheduler,\n",
    "                                device, EPOCHS, SAVE_PATH)\n",
    "\n",
    "    print(f\"Training completed with best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Best validation accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "    # 如果需要单独调用可视化函数，可以在这里添加\n",
    "    # plot_training_history(history, SAVE_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17757,
     "status": "ok",
     "timestamp": 1747036450382,
     "user": {
      "displayName": "Yaojia Wang",
      "userId": "02583595600482354486"
     },
     "user_tz": -480
    },
    "id": "JMOzkj6RahjG",
    "outputId": "3261c095-34fc-47f2-fb4c-a85d0ec65182"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading test data...\n",
      "文件 test.csv 中的标签分布:\n",
      "唯一标签值: [0 1 2 3 4 5]\n",
      "标签计数: {np.int64(0): 580, np.int64(1): 695, np.int64(2): 159, np.int64(3): 275, np.int64(4): 224, np.int64(5): 66}\n",
      "检测到的最大标签值: 5，设置num_labels=6\n",
      "模型权重已加载: model_output/best_model_acc.pt\n",
      "Evaluating model...\n",
      "\n",
      "Evaluation Results:\n",
      "Accuracy: 0.8594\n",
      "Precision: 0.7957\n",
      "Recall: 0.8771\n",
      "F1-Score: 0.8260\n",
      "\n",
      "Example Predictions:\n",
      "\n",
      "Text: This is a great product!\n",
      "Predicted Class: 1 (负面)\n",
      "Confidence: 0.9797\n",
      "Class probabilities:\n",
      "  极负面: 0.0027\n",
      "  负面: 0.9797\n",
      "  略负面: 0.0039\n",
      "  略正面: 0.0073\n",
      "  正面: 0.0044\n",
      "  极正面: 0.0021\n",
      "\n",
      "Text: I did not like the service at all.\n",
      "Predicted Class: 2 (略负面)\n",
      "Confidence: 0.9798\n",
      "Class probabilities:\n",
      "  极负面: 0.0041\n",
      "  负面: 0.0109\n",
      "  略负面: 0.9798\n",
      "  略正面: 0.0043\n",
      "  正面: 0.0003\n",
      "  极正面: 0.0006\n",
      "\n",
      "Text: The food was delicious and the staff were friendly.\n",
      "Predicted Class: 1 (负面)\n",
      "Confidence: 0.9839\n",
      "Class probabilities:\n",
      "  极负面: 0.0012\n",
      "  负面: 0.9839\n",
      "  略负面: 0.0108\n",
      "  略正面: 0.0015\n",
      "  正面: 0.0010\n",
      "  极正面: 0.0016\n",
      "\n",
      "Text: I will never come back to this place again.\n",
      "Predicted Class: 3 (略正面)\n",
      "Confidence: 0.6360\n",
      "Class probabilities:\n",
      "  极负面: 0.1848\n",
      "  负面: 0.0813\n",
      "  略负面: 0.0049\n",
      "  略正面: 0.6360\n",
      "  正面: 0.0908\n",
      "  极正面: 0.0023\n",
      "\n",
      "Text: It was neither good nor bad, just average.\n",
      "Predicted Class: 1 (负面)\n",
      "Confidence: 0.8773\n",
      "Class probabilities:\n",
      "  极负面: 0.1063\n",
      "  负面: 0.8773\n",
      "  略负面: 0.0038\n",
      "  略正面: 0.0087\n",
      "  正面: 0.0017\n",
      "  极正面: 0.0022\n",
      "\n",
      "Text: This exceeded all my expectations!\n",
      "Predicted Class: 1 (负面)\n",
      "Confidence: 0.6192\n",
      "Class probabilities:\n",
      "  极负面: 0.1335\n",
      "  负面: 0.6192\n",
      "  略负面: 0.0039\n",
      "  略正面: 0.2163\n",
      "  正面: 0.0209\n",
      "  极正面: 0.0062\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "# 复用与训练相同的数据集类\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(sentences, truncation=True, padding='max_length', max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 复用与训练相同的增强BERT模型\n",
    "class EnhancedBertForSequenceClassification(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_labels):\n",
    "        super(EnhancedBertForSequenceClassification, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.num_labels = num_labels  # 存储标签数量\n",
    "\n",
    "        # 丰富的分类头部\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, 768),\n",
    "            nn.LayerNorm(768),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(768, 384),\n",
    "            nn.LayerNorm(384),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(384, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        pooled_output = outputs.pooler_output\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def set_class_weights(self, class_weights):\n",
    "        \"\"\"设置类别权重用于损失函数\"\"\"\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "def load_data(file_path, tokenizer, max_length=128):\n",
    "    \"\"\"加载并处理数据，不使用pandas\"\"\"\n",
    "    try:\n",
    "        sentences = []\n",
    "        labels = []\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            # 跳过标题行\n",
    "            next(f)\n",
    "\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "\n",
    "                # 假设CSV格式为: sentence,label\n",
    "                parts = line.split(',')\n",
    "                if len(parts) >= 2:\n",
    "                    sentence = ','.join(parts[:-1])  # 处理句子中可能包含的逗号\n",
    "                    label = int(parts[-1])\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "\n",
    "        dataset = TextDataset(sentences, labels, tokenizer, max_length)\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"数据加载错误: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_data_loader(dataset, batch_size, sampler_type='sequential'):\n",
    "    \"\"\"创建数据加载器\"\"\"\n",
    "    sampler = torch.utils.data.SequentialSampler(dataset)\n",
    "    return DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "def calculate_metrics_multiclass_improved(preds, labels):\n",
    "    \"\"\"计算多分类任务的评估指标\"\"\"\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    # 计算每个类别的精确率、召回率和F1分数\n",
    "    metrics = {'accuracy': np.sum(pred_flat == labels_flat) / len(labels_flat)}\n",
    "\n",
    "    classes = np.unique(np.concatenate([pred_flat, labels_flat]))\n",
    "\n",
    "    # 计算宏平均指标\n",
    "    precision_sum = 0\n",
    "    recall_sum = 0\n",
    "    f1_sum = 0\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    for cls in classes:\n",
    "        tp = np.sum((pred_flat == cls) & (labels_flat == cls))\n",
    "        fp = np.sum((pred_flat == cls) & (labels_flat != cls))\n",
    "        fn = np.sum((pred_flat != cls) & (labels_flat == cls))\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        precision_sum += precision\n",
    "        recall_sum += recall\n",
    "        f1_sum += f1\n",
    "\n",
    "    metrics['precision'] = precision_sum / n_classes\n",
    "    metrics['recall'] = recall_sum / n_classes\n",
    "    metrics['f1'] = f1_sum / n_classes\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    \"\"\"评估模型性能\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_input_mask = batch['attention_mask'].to(device)\n",
    "        b_labels = batch['labels'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        all_preds.append(logits)\n",
    "        all_labels.append(label_ids)\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    return calculate_metrics_multiclass_improved(all_preds, all_labels)\n",
    "\n",
    "def predict_text(model, tokenizer, text, device, max_length=128):\n",
    "    \"\"\"预测单个文本的情感\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    inputs = tokenizer(text, truncation=True, padding='max_length',\n",
    "                      max_length=max_length, return_tensors='pt')\n",
    "\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "    confidence, predicted_class = torch.max(probs, dim=1)\n",
    "\n",
    "    predicted_class = predicted_class.item()\n",
    "    confidence = confidence.item()\n",
    "    probs = probs.cpu().numpy()[0]\n",
    "\n",
    "    return {\n",
    "        'predicted_class': predicted_class,\n",
    "        'confidence': confidence,\n",
    "        'probabilities': probs\n",
    "    }\n",
    "\n",
    "def inspect_dataset(file_path):\n",
    "    \"\"\"检查数据集中的标签分布\"\"\"\n",
    "    labels = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        next(f)  # 跳过标题行\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            if len(parts) >= 2:\n",
    "                label = int(parts[-1])\n",
    "                labels.append(label)\n",
    "\n",
    "    unique_labels = np.unique(labels)\n",
    "    counts = {label: labels.count(label) for label in unique_labels}\n",
    "\n",
    "    print(f\"文件 {file_path} 中的标签分布:\")\n",
    "    print(f\"唯一标签值: {unique_labels}\")\n",
    "    print(f\"标签计数: {counts}\")\n",
    "\n",
    "    return unique_labels, counts\n",
    "\n",
    "def main():\n",
    "    # 设置参数\n",
    "    MODEL_PATH = 'model_output/best_model_acc.pt'  # 使用最高准确率的模型\n",
    "    TEST_FILE = 'test.csv'\n",
    "    MAX_LENGTH = 128\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # 加载数据集并检查标签分布\n",
    "    print(\"Loading test data...\")\n",
    "    test_dataset = load_data(TEST_FILE, tokenizer, MAX_LENGTH)\n",
    "\n",
    "    if test_dataset is None:\n",
    "        print(\"测试数据加载失败，程序退出\")\n",
    "        return\n",
    "\n",
    "    test_labels, test_counts = inspect_dataset(TEST_FILE)\n",
    "\n",
    "    # 确定标签数量\n",
    "    num_labels = max(test_labels) + 1\n",
    "    print(f\"检测到的最大标签值: {num_labels-1}，设置num_labels={num_labels}\")\n",
    "\n",
    "    # 创建与训练时相同的模型架构\n",
    "    model = EnhancedBertForSequenceClassification('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "    # 加载训练好的模型权重\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "        print(f\"模型权重已加载: {MODEL_PATH}\")\n",
    "    else:\n",
    "        print(f\"模型文件不存在: {MODEL_PATH}\")\n",
    "        return\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    print(\"Evaluating model...\")\n",
    "    test_dataloader = create_data_loader(test_dataset, batch_size=BATCH_SIZE)\n",
    "    metrics = evaluate_model(model, test_dataloader, device)\n",
    "\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {metrics['f1']:.4f}\")\n",
    "\n",
    "    # 示例测试文本\n",
    "    print(\"\\nExample Predictions:\")\n",
    "    test_texts = [\n",
    "        \"This is a great product!\",\n",
    "        \"I did not like the service at all.\",\n",
    "        \"The food was delicious and the staff were friendly.\",\n",
    "        \"I will never come back to this place again.\",\n",
    "        \"It was neither good nor bad, just average.\",\n",
    "        \"This exceeded all my expectations!\"\n",
    "    ]\n",
    "\n",
    "    class_mappings = {\n",
    "        0: \"极负面\",\n",
    "        1: \"负面\",\n",
    "        2: \"略负面\",\n",
    "        3: \"略正面\",\n",
    "        4: \"正面\",\n",
    "        5: \"极正面\"\n",
    "    }\n",
    "\n",
    "    for text in test_texts:\n",
    "        result = predict_text(model, tokenizer, text, device, MAX_LENGTH)\n",
    "        predicted_class = result['predicted_class']\n",
    "        sentiment = class_mappings.get(predicted_class, f\"类别{predicted_class}\")\n",
    "\n",
    "        print(f\"\\nText: {text}\")\n",
    "        print(f\"Predicted Class: {predicted_class} ({sentiment})\")\n",
    "        print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "\n",
    "        # 打印各类别概率\n",
    "        print(\"Class probabilities:\")\n",
    "        for i, prob in enumerate(result['probabilities']):\n",
    "            sent = class_mappings.get(i, f\"类别{i}\")\n",
    "            print(f\"  {sent}: {prob:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNlDqtVhzwyeyV38fua/Ug3",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
